{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'ptl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-d3af0e8ee117>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mptl\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'ptl'"
     ]
    }
   ],
   "source": [
    "from ptl import get\n",
    "get.setup(debug=True, driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "get.N_FAIL_REQUEST=1\n",
    "def soups(rows, folder, t_id):\n",
    "    for row in rows:\n",
    "        try:    \n",
    "            get.log('> GET '+row['URL'])\n",
    "            rq = get.get(row['URL'])\n",
    "            row['ADDRESS']=row['URL']\n",
    "            row['HEADERS']=rq.headers\n",
    "            row['CONTENT TYPE']=rq.headers['Content-Type'] if 'Content-Type' in rq.headers else ''\n",
    "            #row['CONTENT LENGTH']=rq.headers['Content-Length'] if 'Content-Length' in rq.headers else ''\n",
    "            #row['SERVER']=rq.headers['Server']\n",
    "            row['DATE']=rq.headers['Date']\n",
    "            #row['ENCODE']=rq.encoding\n",
    "            row['RESPONSE TIME']=rq.elapsed\n",
    "            row['REDIRECT TYPE']='NONE' if not rq.is_redirect else 'PERMANENT' if rq.is_permanent_redirect else 'TEMPERARY'\n",
    "            row['REDIRECT']=rq.is_redirect\n",
    "            row['STATUS']=rq.reason\n",
    "            row['STATUS CODE']=rq.status_code\n",
    "            row['FINAL REACH URL']=rq.url\n",
    "            row['HTTP/HTTPS']='HTTPS' if 'https' in rq.url[:10] else 'HTTP'\n",
    "            row['SOUP'] = get.bs(rq.content, 'html.parser')\n",
    "            check('title', row)\n",
    "            check('h1', row)\n",
    "            check('meta', row)\n",
    "            check('img', row)\n",
    "            #check('h2', row)\n",
    "            #check('title', row)\n",
    "            #check('link', row)\n",
    "        except: get.pe()\n",
    "def get_status(_len):\n",
    "    if _len>160: return \"TOO LONG\"\n",
    "    if _len<50: return 'TOO SHORT' \n",
    "    return 'GOOD'\n",
    "def check(tag, row):\n",
    "    try:\n",
    "        tags=row['SOUP'].findAll(tag)\n",
    "        if tag=='img':\n",
    "            row['Number of images'.upper()] = len(tags)\n",
    "            imgs_with_tag=[img for img in tags if 'alt' in img]\n",
    "\n",
    "            row['Number of with alt-tag'.upper()] = len(imgs_with_tag)\n",
    "            row['Number of without alt-tag'.upper()] = len(tags) - len(imgs_with_tag)\n",
    "            alt_tags=numpy.array([img['alt'] for img in imgs_with_tag])\n",
    "            row['alt-tag Minimum length'.upper()] = alt_tags.min()\n",
    "            row['alt-tag Average length'.upper()] = alt_tags.mean()\n",
    "            row['alt-tag Maximum length'.upper()] = alt_tags.max()\n",
    "            df = get.DataFrame(alt_tags)\n",
    "            row['Number of Alt-Tags that are duplicated'.upper()] = len(df[df.duplicated])\n",
    "            return\n",
    "        for i, _tag in enumerate(tags):\n",
    "            if tag=='meta':\n",
    "                if _tag.has_attr('name') and (_tag['name'].lower()=='description' or\n",
    "                                              _tag['name'].lower()=='keywords'):\n",
    "                    name='META '+_tag['name']\n",
    "                    row[name] = _tag['content']\n",
    "                    _len = len(row[name])\n",
    "                    row[name+' length'.upper()] = _len\n",
    "                    row[name+' status'.upper()] = get_status(_len)\n",
    "\n",
    "            elif tag=='link':\n",
    "                if _tag.has_attr('rel') and _tag['rel'].lower()=='canonical': \n",
    "                    name=_tag['rel'].upper()\n",
    "                    row[name.upper()] = _tag['href']\n",
    "                    row[name.upper()+' length'.upper()] = len(_tag['href'])\n",
    "            else:\n",
    "                name=tag.upper()\n",
    "                if 'h' in tag:\n",
    "                    name=name+' '+str(i)\n",
    "                row[name]=_tag.text\n",
    "                _len = len(row[name])\n",
    "                row[name+' length'.upper()]=_len\n",
    "                row[name+' status'.upper()] = get_status(_len)\n",
    "        if 'META DESCRIPTION' not in row.keys:\n",
    "            row['META DESCRIPTION']= '[Missing meta description]'\n",
    "    except: get.pe()\n",
    "def distinct(alist):\n",
    "    return list(dict.fromkeys(alist))\n",
    "url='https://nativeenergy.com/'\n",
    "try:\n",
    "    robots = get.get(url+'/robots.txt')\n",
    "    if not robots:\n",
    "        get.log('> robots.txt not found')\n",
    "except: \n",
    "    get.log('> robots.txt not found')\n",
    "    get.pe()\n",
    "\n",
    "VISISTED=[]\n",
    "STACK=[url]\n",
    "N_THREADS= 5\n",
    "THRESHOLD= 3 \n",
    "DOWNLOADED=[]\n",
    "iteration=1\n",
    "while len(STACK)!=0:\n",
    "    get.log('> Iteration:  '+str(iteration)+', stack size: '+str(len(STACK)))\n",
    "    get.log('> Downloaded: '+str(len(DOWNLOADED)))\n",
    "    stack=[{'URL':url} for url in STACK if '.pdf' not in url and '/members/' not in url]\n",
    "    #print('> test'+str(get.split(STACK, N_THREADS)))\n",
    "    if len(stack)/N_THREADS > THRESHOLD:\n",
    "        N_THREADS=get.ceil(len(stack)/THRESHOLD)\n",
    "    get.log('> Getting content')\n",
    "    soups(stack, '', '')\n",
    "    #get.run_threads(get.split(stack, N_THREADS), soups, folder='nodriver')\n",
    "    VISISTED=VISISTED+STACK\n",
    "    new_links=[]\n",
    "    for item in stack:\n",
    "        if 'SOUP' in item and item['SOUP']:\n",
    "            item['a']=[a for a in get.bes(None, div='a', pr=item['SOUP']) if 'href' in str(a)]\n",
    "            links=[a['href'] for a in item['a'] if len(a['href'])> 3]\n",
    "            links=[(url+a).replace('//','/').replace(':/', '://') for a in links \n",
    "                   if '//' not in a and \n",
    "                   '?' not in a and \n",
    "                   'javascript' not in a]\n",
    "            links=distinct(links)\n",
    "            new_links=new_links+links\n",
    "    get.log('> New links:  '+str(len(new_links)))\n",
    "    DOWNLOADED=DOWNLOADED+stack\n",
    "    STACK = list(set(new_links)-set(new_links).intersection(set(VISISTED)))[:2]\n",
    "    iteration+=1\n",
    "    if iteration==3:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
