{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptl import get\n",
    "get.setup(debug=True, driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get.setup(debug=True, driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pages=[]\n",
    "stack=[]\n",
    "visited=[]\n",
    "def analyze_page(url):\n",
    "    global stack, visited, pages\n",
    "    result={}\n",
    "    soup=get.soup(url, get=True)\n",
    "    visited.append(url)\n",
    "    print('Progress:\\t\\t'+str(len(pages))+'/'+str(len(stack)))\n",
    "    result['a']=[a for a in get.bes(div='a', parent=soup) if 'href' in str(a) and len(a['href'])> 3 and get.dmr(url) in a['href']]\n",
    "    links=[a['href'] for a in result['a']]\n",
    "    links=distinct(links)\n",
    "    stack=distinct(stack+links)\n",
    "    pages.append({'URL':url, 'SOUP':soup})\n",
    "get.START_TIME=get.dtn()\n",
    "print('Start time: \\t'+get.START_TIME)\n",
    "analyze_page('https://ukuleleunderground.com/')\n",
    "for url in stack:\n",
    "    analyze_page(url)\n",
    "notvisited=list(set(stack)-set(stack).intersection(set(visited)))\n",
    "while len(notvisited)!=0:\n",
    "    for url in notvisited:\n",
    "        #multi threadable\n",
    "        analyze_page(url)\n",
    "    notvisited=list(set(stack)-set(stack).intersection(set(visited)))\n",
    "print('Start time: \\t'+get.START_TIME+'\\nFinish time: \\t'+get.dtn())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def distinct(alist):\n",
    "    return list(dict.fromkeys(alist))\n",
    "url='https://ukuleleunderground.com/'\n",
    "VISISTED=[]\n",
    "STACK=[url]\n",
    "N_THREADS= 5\n",
    "THRESHOLD= 3 \n",
    "DOWNLOADED=[]\n",
    "iteration=0\n",
    "while len(STACK)!=0:\n",
    "    get.log('> Iteration: '+str(iteration)+', stack size: '+str(len(STACK)))\n",
    "    get.log('> Downloaded:'+str(len(DOWNLOADED)))\n",
    "    stack=[{'URL':url} for url in STACK if '.pdf' not in url and '/members/' not in url]\n",
    "    #print('> test'+str(get.split(STACK, N_THREADS)))\n",
    "    if len(stack)/N_THREADS > THRESHOLD:\n",
    "        N_THREADS=get.ceil(len(stack)/THRESHOLD)\n",
    "    get.run_threads(get.split(stack, N_THREADS), get.soups, folder='nodriver')\n",
    "    get.log('> Downloaded data')\n",
    "    VISISTED=VISISTED+STACK\n",
    "    new_links=[]\n",
    "    for item in stack:\n",
    "        if item['SOUP']:\n",
    "            item['a']=[a for a in get.bes(None, div='a', parent=item['SOUP']) if 'href' in str(a)]\n",
    "            links=[a['href'] for a in item['a'] if len(a['href'])> 3 and get.dmr(url) in a['href']]\n",
    "            new_links=new_links+distinct(links)\n",
    "    get.log('> New links: '+str(len(new_links)))\n",
    "    DOWNLOADED=DOWNLOADED+stack[:500]\n",
    "    STACK = list(set(new_links)-set(new_links).intersection(set(VISISTED)))[:1000]\n",
    "    iteration+=1\n",
    "    if iteration==2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get.save(DOWNLOADED, get.dmn(url).upper()+'/'+get.START_TIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "req=get.make_request(STACK[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get.DataFrame(DOWNLOADED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr(tag):\n",
    "    _tag=get.bes(None, div=tag, parent=row['SOUP'])\n",
    "    print(tag+':\\t', len(_tag) if _tag else 0)\n",
    "    for i in _tag:\n",
    "        try:\n",
    "            if tag=='meta':\n",
    "                meta={}\n",
    "                if i.has_attr('name'):    \n",
    "                    meta[i['name']] = i['content']\n",
    "                elif i.has_attr('charset'):\n",
    "                    meta['charset'] = i['charset']\n",
    "                print(meta)\n",
    "            else:\n",
    "                meta=i.text\n",
    "            print(len(meta),'\\t'+ str(meta))\n",
    "        except: get.pe(str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in df.iloc[0]['SOUP'].findAll('img'):\n",
    "    if i.has_attr('alt'):\n",
    "        if len(i['alt'])>1:\n",
    "            print('Image have alt text')\n",
    "        else:\n",
    "            print('Image have alt tag but no alt text:\\n', i['src'])\n",
    "    else:\n",
    "        print('Image doesn\\'t have a alt tag:', i['src'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_duplication(soup):\n",
    "    \n",
    "    df = get.df(soup.text.split('\\n')).duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.iloc[0]['SOUP'].findAll('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_df(tag):\n",
    "    text=[{'Text': row.text.strip(), 'Source':row} for row in df.iloc[0]['SOUP'].findAll(tag)]\n",
    "    return get.DataFrame(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_df('video').groupby('Text').count() if len(get_df('video'))>0 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_df('p').groupby('Text').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_df('h1').groupby('Text').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_df('h2').groupby('Text').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get_df('div').groupby('Text').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i, row in df.iterrows():\n",
    "    print('URL:\\t',row['URL'])\n",
    "    pr('title')    \n",
    "    pr('meta')    \n",
    "    pr('h1')    \n",
    "    pr('h2')   \n",
    "    pr('video')    \n",
    "    pr('video')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rq = get.get(url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = get.bs(rq.content, 'html.parser')\n",
    "rs = [{'HEADERS':rq.headers,\n",
    "       'CONTENT TYPE':rq.headers['Content-Type'],\n",
    "       'CONTENT LENGTH':rq.headers['Content-Length'],\n",
    "       'SERVER':rq.headers['Server'],\n",
    "       'DATE':rq.headers['Date'],\n",
    "      'ENCODE':rq.encoding,\n",
    "      'COOKIES':rq.cookies,\n",
    "      'RESPONSE TIME':rq.elapsed,\n",
    "      'HISTORY':rq.history,\n",
    "      'REDIRECT TYPE':'NONE' if not rq.is_redirect else 'PERMANENT' if rq.is_permanent_redirect else 'TEMPERARY',\n",
    "      'REDIRECT':rq.is_redirect,\n",
    "      'LINKS':rq.links,\n",
    "      'NEXT LINK':rq.next,\n",
    "      'STATUS':rq.reason,\n",
    "       'STATUS CODE':rq.status_code,\n",
    "      'FINAL REACH URL':rq.url,\n",
    "     }]\n",
    "checkfor('title', soup, rs[0])\n",
    "checkfor('h1', soup, rs[0])\n",
    "checkfor('h2', soup, rs[0])\n",
    "checkfor('title', soup, rs[0])\n",
    "checkfor('meta', soup, rs[0])\n",
    "checkfor('link', soup, rs[0])\n",
    "get.DataFrame(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkfor(tag, soup, rs):\n",
    "    tags=soup.findAll(tag)\n",
    "    for i, _tag in enumerate(tags):\n",
    "        if tag=='meta':\n",
    "            if _tag.has_attr('name'):    \n",
    "                rs[_tag['name'].upper()] = _tag['content']\n",
    "                rs[_tag['name'].upper()+' LENGTH'] = len(_tag['content'])\n",
    "        elif tag=='link':\n",
    "            if _tag.has_attr('rel'):    \n",
    "                rs[_tag['rel'][0].upper()] = _tag['href']\n",
    "                rs[_tag['rel'][0].upper()+' LENGTH'] = len(_tag['href'])\n",
    "        else:\n",
    "            rs[tag.upper()+' '+str(i)]=_tag.text\n",
    "            rs[tag.upper()+' '+str(i)+' LENGTH']=len(_tag.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get.save(get.DataFrame(rs), 'sf_test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get.soup('https://ukuleleunderground.com/page-sitemap.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls=[a.text for a in get.bes(None, 'loc')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get.soup('https://ukuleleunderground.com/sitemap_index.xml')\n",
    "locs=[loc.text for loc in get.bes(None, 'loc')]\n",
    "urls=[]\n",
    "for loc in locs:\n",
    "    get.soup(loc)\n",
    "    print(loc)\n",
    "    urls = urls+ [{'URL':s.text, 'SOURCE':loc} for s in get.bes(None, 'loc')]\n",
    "print(len(urls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(get.DataFrame(urls)['URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distinct(alist):\n",
    "    return list(dict.fromkeys(alist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distinct(get.DataFrame(urls)['URL'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data= get.DataFrame(urls)\n",
    "data.groupby('URL').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data[data.duplicated(['SOURCE'])].groupby('SOURCE').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['URL']=='https://ukuleleunderground.com/courses/applied-music-theory/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "grouped_df=data[data.duplicated(['URL'])].groupby('SOURCE')\n",
    "for key, item in grouped_df:\n",
    "    for v,i in item.iterrows():\n",
    "        duplicate=data[data['URL']==i['URL']]\n",
    "        print('DUPLICATE:',duplicate['URL'].iloc[0])\n",
    "        print('SOURCE  0:',duplicate['SOURCE'].iloc[0])\n",
    "        print('SOURCE  1:',duplicate['SOURCE'].iloc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ptl import get\n",
    "get.setup(debug=True, driver=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "get.N_FAIL_REQUEST=1\n",
    "def soups(rows, folder, t_id):\n",
    "    for row in rows:\n",
    "        try:    \n",
    "            get.log('> GET '+row['URL'])\n",
    "            rq = get.get(row['URL'])\n",
    "            row['ADDRESS']=row['URL']\n",
    "            row['HEADERS']=rq.headers\n",
    "            row['CONTENT TYPE']=rq.headers['Content-Type'] if 'Content-Type' in rq.headers else ''\n",
    "            #row['CONTENT LENGTH']=rq.headers['Content-Length'] if 'Content-Length' in rq.headers else ''\n",
    "            #row['SERVER']=rq.headers['Server']\n",
    "            row['DATE']=rq.headers['Date']\n",
    "            #row['ENCODE']=rq.encoding\n",
    "            row['RESPONSE TIME']=rq.elapsed\n",
    "            row['REDIRECT TYPE']='NONE' if not rq.is_redirect else 'PERMANENT' if rq.is_permanent_redirect else 'TEMPERARY'\n",
    "            row['REDIRECT']=rq.is_redirect\n",
    "            row['STATUS']=rq.reason\n",
    "            row['STATUS CODE']=rq.status_code\n",
    "            row['FINAL REACH URL']=rq.url\n",
    "            row['HTTP/HTTPS']='HTTPS' if 'https' in rq.url[:10] else 'HTTP'\n",
    "            row['SOUP'] = get.bs(rq.content, 'html.parser')\n",
    "            check('title', row)\n",
    "            check('h1', row)\n",
    "            check('meta', row)\n",
    "            check('img', row)\n",
    "            #check('h2', row)\n",
    "            #check('title', row)\n",
    "            #check('link', row)\n",
    "        except: get.pe()\n",
    "def get_status(_len):\n",
    "    if _len>160: return \"TOO LONG\"\n",
    "    if _len<50: return 'TOO SHORT' \n",
    "    return 'GOOD'\n",
    "def check(tag, row):\n",
    "    try:\n",
    "        tags=row['SOUP'].findAll(tag)\n",
    "        if tag=='img':\n",
    "            row['Number of images'.upper()] = len(tags)\n",
    "            imgs_with_tag=[img for img in tags if 'alt' in img]\n",
    "\n",
    "            row['Number of with alt-tag'.upper()] = len(imgs_with_tag)\n",
    "            row['Number of without alt-tag'.upper()] = len(tags) - len(imgs_with_tag)\n",
    "            alt_tags=numpy.array([img['alt'] for img in imgs_with_tag])\n",
    "            row['alt-tag Minimum length'.upper()] = alt_tags.min()\n",
    "            row['alt-tag Average length'.upper()] = alt_tags.mean()\n",
    "            row['alt-tag Maximum length'.upper()] = alt_tags.max()\n",
    "            df = get.DataFrame(alt_tags)\n",
    "            row['Number of Alt-Tags that are duplicated'.upper()] = len(df[df.duplicated])\n",
    "            return\n",
    "        for i, _tag in enumerate(tags):\n",
    "            if tag=='meta':\n",
    "                if _tag.has_attr('name') and (_tag['name'].lower()=='description' or\n",
    "                                              _tag['name'].lower()=='keywords'):\n",
    "                    name='META '+_tag['name']\n",
    "                    row[name] = _tag['content']\n",
    "                    _len = len(row[name])\n",
    "                    row[name+' length'.upper()] = _len\n",
    "                    row[name+' status'.upper()] = get_status(_len)\n",
    "\n",
    "            elif tag=='link':\n",
    "                if _tag.has_attr('rel') and _tag['rel'].lower()=='canonical': \n",
    "                    name=_tag['rel'].upper()\n",
    "                    row[name.upper()] = _tag['href']\n",
    "                    row[name.upper()+' length'.upper()] = len(_tag['href'])\n",
    "            else:\n",
    "                name=tag.upper()\n",
    "                if 'h' in tag:\n",
    "                    name=name+' '+str(i)\n",
    "                row[name]=_tag.text\n",
    "                _len = len(row[name])\n",
    "                row[name+' length'.upper()]=_len\n",
    "                row[name+' status'.upper()] = get_status(_len)\n",
    "        if 'META DESCRIPTION' not in row.keys:\n",
    "            row['META DESCRIPTION']= '[Missing meta description]'\n",
    "    except: get.pe()\n",
    "def distinct(alist):\n",
    "    return list(dict.fromkeys(alist))\n",
    "url='https://nativeenergy.com/'\n",
    "try:\n",
    "    robots = get.get(url+'/robots.txt')\n",
    "    if not robots:\n",
    "        get.log('> robots.txt not found')\n",
    "except: \n",
    "    get.log('> robots.txt not found')\n",
    "    get.pe()\n",
    "\n",
    "VISISTED=[]\n",
    "STACK=[url]\n",
    "N_THREADS= 5\n",
    "THRESHOLD= 3 \n",
    "DOWNLOADED=[]\n",
    "iteration=1\n",
    "while len(STACK)!=0:\n",
    "    get.log('> Iteration:  '+str(iteration)+', stack size: '+str(len(STACK)))\n",
    "    get.log('> Downloaded: '+str(len(DOWNLOADED)))\n",
    "    stack=[{'URL':url} for url in STACK if '.pdf' not in url and '/members/' not in url]\n",
    "    #print('> test'+str(get.split(STACK, N_THREADS)))\n",
    "    if len(stack)/N_THREADS > THRESHOLD:\n",
    "        N_THREADS=get.ceil(len(stack)/THRESHOLD)\n",
    "    get.log('> Getting content')\n",
    "    soups(stack, '', '')\n",
    "    #get.run_threads(get.split(stack, N_THREADS), soups, folder='nodriver')\n",
    "    VISISTED=VISISTED+STACK\n",
    "    new_links=[]\n",
    "    for item in stack:\n",
    "        if 'SOUP' in item and item['SOUP']:\n",
    "            item['a']=[a for a in get.bes(None, div='a', pr=item['SOUP']) if 'href' in str(a)]\n",
    "            links=[a['href'] for a in item['a'] if len(a['href'])> 3]\n",
    "            links=[(url+a).replace('//','/').replace(':/', '://') for a in links \n",
    "                   if '//' not in a and \n",
    "                   '?' not in a and \n",
    "                   'javascript' not in a]\n",
    "            links=distinct(links)\n",
    "            new_links=new_links+links\n",
    "    get.log('> New links:  '+str(len(new_links)))\n",
    "    DOWNLOADED=DOWNLOADED+stack\n",
    "    STACK = list(set(new_links)-set(new_links).intersection(set(VISISTED)))[:2]\n",
    "    iteration+=1\n",
    "    if iteration==3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_len = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unidecode import unidecode\n",
    "def FormatString(s):\n",
    "    if isinstance(s, unicode):\n",
    "        try:\n",
    "            s.encode('ascii')\n",
    "            return s\n",
    "        except:\n",
    "            return unidecode(s)\n",
    "    else:\n",
    "        return s\n",
    "\n",
    "#df2 = df1.applymap(FormatString) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "grouped_df=df[df.duplicated(['TITLE 0'])].groupby('TITLE 0')\n",
    "for key, item in grouped_df:\n",
    "    for v,i in item.iterrows():\n",
    "        duplicate=df[df['TITLE 0']==i['TITLE 0']]\n",
    "        print('DUPLICATE:'+str(duplicate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[df.duplicated(['TITLE 0'])].groupby('URL').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = get.DataFrame(DOWNLOADED).applymap(lambda x: x.encode('unicode_escape').decode('utf-8') if isinstance(x, str) else x)\n",
    "df = df.sort_values(['DESCRIPTION LENGTH', 'TITLE 0 LENGTH'], ascending=False)\n",
    "get.save(df, 'SFA/test', ['SOUP'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "writer = pd.ExcelWriter('pandas_simple.xlsx', engine='xlsxwriter')\n",
    "df.to_excel(writer, sheet_name='Sheet1')\n",
    "writer.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get.isdir('SFA')\n",
    "df = df.sort_values(['DESCRIPTION LENGTH', 'TITLE 0 LENGTH'], ascending=False)\n",
    "df.to_csv('SFA/test.csv', encoding='utf8', columns=df.columns.drop(['SOUP']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in stack:\n",
    "    if 'SOUP' in item and item['SOUP']:\n",
    "        item['a']=[a for a in get.bes(None, div='a', pr=item['SOUP']) if 'href' in str(a)]\n",
    "        links=[a['href'] for a in item['a'] if len(a['href'])> 3]\n",
    "        links=[(url+a).replace('//','/').replace(':/', '://') for a in links \n",
    "               if '//' not in a and \n",
    "               '?' not in a and \n",
    "               'javascript' not in a]\n",
    "        links=distinct(links)\n",
    "        new_links=new_links+links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "get.get('https://nativeenergy.comassets/files/Light_Boxes/PSP_CurtisPackaging.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item['a']=[a for a in get.bes(None, div='a', pr=item['SOUP']) if 'href' in str(a)]\n",
    "links=[root+a['href'] for a in item['a'] if len(a['href'])> 3 and get.dmr(url) in a['href']]\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root=get.dmr(url)\n",
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "links=[a['href'] for a in item['a'] if len(a['href'])> 3]\n",
    "links=distinct([root+a for a in links if '/' in a and '//' not in a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[a['href'] for a in get.bes(None, 'a') if a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get.bes(None, 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "robots=None\n",
    "def request(url):\n",
    "    global robots\n",
    "    if not robots:\n",
    "        robots=get.dmr(url)+'/robots.txt'\n",
    "        robots =get.get(robots).content\n",
    "    #print('> GET:', url)\n",
    "    rq = get.get(url)\n",
    "    soup = get.bs(rq.content, 'html.parser')\n",
    "    rs = {'HEADERS':rq.headers,\n",
    "       'CONTENT TYPE':rq.headers['Content-Type'] if 'Content-Type' in rq.headers else '',\n",
    "       'CONTENT LENGTH':rq.headers['Content-Length'] if 'Content-Length' in rq.headers else '',\n",
    "       'SERVER':rq.headers['Server'],\n",
    "       'DATE':rq.headers['Date'],\n",
    "      'ENCODE':rq.encoding,\n",
    "      'COOKIES':rq.cookies,\n",
    "      'RESPONSE TIME':rq.elapsed,\n",
    "      'HISTORY':rq.history,\n",
    "      'REDIRECT TYPE':'NONE' if not rq.is_redirect else 'PERMANENT' if rq.is_permanent_redirect else 'TEMPERARY',\n",
    "      'REDIRECT':rq.is_redirect,\n",
    "      'LINKS':rq.links,\n",
    "      'NEXT LINK':rq.next,\n",
    "      'STATUS':rq.reason,\n",
    "       'STATUS CODE':rq.status_code,\n",
    "      'FINAL REACH URL':rq.url,\n",
    "           'HTTP/HTTPS': 'HTTPS' if 'https' in rq.url[:10] else 'HTTP',\n",
    "           'ROBOTS.TXT':robots,\n",
    "         }\n",
    "    checkfor('title', soup, rs)\n",
    "    checkfor('h1', soup, rs)\n",
    "    checkfor('h2', soup, rs)\n",
    "    checkfor('title', soup, rs)\n",
    "    checkfor('meta', soup, rs)\n",
    "    checkfor('link', soup, rs)\n",
    "    return rs\n",
    "rows=[]\n",
    "total=len(data['URL'])\n",
    "for i, url in enumerate(data['URL']):\n",
    "    print(i,'/',total,'>>>',url)\n",
    "    rows.append(request(url))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data['URL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rq = get.get('https://ukuleleunderground.com/course-level/beginner/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get.save(get.DataFrame(rows), 'test_1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "get.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rq.headers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The apparent encoding, provided by the chardet library.\n",
    "rq.apparent_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoding to decode with when accessing r.text.\n",
    "rq.encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A CookieJar of Cookies the server sent back.\n",
    "rq.cookies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Response status code\n",
    "rq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rq.status_code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The amount of time elapsed between sending the request and\n",
    "#the arrival of the response (as a timedelta). \n",
    "#This property specifically measures the time taken between \n",
    "#sending the first byte of the request and finishing parsing the headers. \n",
    "#It is therefore unaffected by consuming the response content or the \n",
    "#value of the stream keyword argument.\n",
    "rq.elapsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#A list of Response objects from the history of the Request.\n",
    "#Any redirect responses will end up here. \n",
    "#The list is sorted from the oldest to the most recent request.\n",
    "rq.history "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#True if this Response one of the permanent versions of redirect.\n",
    "rq.is_permanent_redirect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#True if this Response is a well-formed HTTP redirect \n",
    "#that could have been processed automatically \n",
    "rq.is_redirect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns the parsed header links of the response, if any.\n",
    "rq.links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns a PreparedRequest for the next request \n",
    "#in a redirect chain, if there is one.\n",
    "rq.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returns True if status_code is less than 400.\n",
    "rq.ok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rq.reason "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Final URL location of Response. In case got redirected.\n",
    "rq.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
